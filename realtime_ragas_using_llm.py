import os
import asyncio
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_recall
from datasets import Dataset

# Set environment variable for OpenAI (replace with your actual key)
os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"

# 1. Load + split PDF
loader = PyPDFLoader("/mnt/ddrive/RAG/realtime_ragas_using_llm.py")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

# Compute full context for ground truth generation (assuming document fits in context window)
def format_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])

full_context = format_docs(docs)

# 2. Embed and store in vectorstore
embedding = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(split_docs, embedding)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 3. Build retriever pipeline step
retrieval_step = RunnableLambda(lambda x: {
    "context": retriever.get_relevant_documents(x["question"]),
    "question": x["question"]
})

# 4. Format documents into a single string for RAG
format_step = RunnableLambda(lambda x: {
    "context": format_docs(x["context"]),
    "question": x["question"]
})

# 5. Prompt template for RAG answer
prompt = PromptTemplate.from_template("""
You are a helpful assistant. Use the following context to answer the question.

Context:
{context}

Question:
{question}
""")

# 6. Prompt template for generating ground truth (using full context)
ground_truth_prompt = PromptTemplate.from_template("""
You are an expert assistant. Use the entire provided document context to generate a complete and accurate answer to the question.

Full Document Context:
{context}

Question:
{question}

Provide the ideal, ground truth answer based on the full context:
""")

# 7. LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 8. RAG chain
llm_chain = prompt | llm

# 9. Ground truth chain
ground_truth_chain = ground_truth_prompt | llm

# 10. Combine the full RAG pipeline
rag_pipeline = retrieval_step | format_step | llm_chain

# 11. RAGAS evaluation function
def prepare_ragas_dataset(question, contexts, answer, ground_truth):
    return Dataset.from_dict({
        "question": [question],
        "contexts": [contexts],
        "answer": [answer],
        "ground_truth": [ground_truth]
    })

async def evaluate_with_ragas(question, contexts, answer, ground_truth):
    dataset = prepare_ragas_dataset(question, contexts, answer, ground_truth)
    result = evaluate(
        dataset=dataset,
        metrics=[faithfulness, answer_relevancy, context_relevancy, context_recall],
    )
    return result

# 12. Run loop with real-time evaluation
async def main():
    while True:
        query = input("Ask a question (or type 'exit'): ")
        if query.lower() == "exit":
            break

        # Generate ground truth using full context
        ground_truth_result = ground_truth_chain.invoke({"context": full_context, "question": query})
        ground_truth = ground_truth_result.content
        print("Generated Ground Truth:", ground_truth)

        # Run RAG pipeline
        result = rag_pipeline.invoke({"question": query})
        answer = result.content
        print("Bot (RAG Answer):", answer)

        # Retrieve contexts for RAGAS evaluation
        retrieved_docs = retriever.get_relevant_documents(query)
        contexts = [doc.page_content for doc in retrieved_docs]

        # Perform RAGAS evaluation
        scores = await evaluate_with_ragas(query, contexts, answer, ground_truth)
        print("\nReal-Time Evaluation Scores:")
        print(f"Faithfulness: {scores['faithfulness']:.3f}")
        print(f"Answer Relevancy: {scores['answer_relevancy']:.3f}")
        print(f"Context Relevancy: {scores['context_relevancy']:.3f}")
        print(f"Context Recall: {scores['context_recall']:.3f}\n")

# Run the async main function
if __name__ == "__main__":
    asyncio.run(main())



#this was generated by shahana loosu madri